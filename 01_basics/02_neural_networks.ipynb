{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae3d170",
   "metadata": {},
   "source": [
    "# 02. Neural Networks\n",
    "\n",
    "In this section, we will learn about **neural networks** and focus more on their architecture and training process.\n",
    "\n",
    "In the next notebook, [03. Neural Networks in PyTorch](03_neural_networks_in_pytorch.ipynb), we will implement a neural network using PyTorch.\n",
    "\n",
    "Let's say you want to predict if you will pass an exam `(1)` or not `(0)`. You have the following data:\n",
    "\n",
    "- Hours studied (x1)\n",
    "- How smart you are (x2)\n",
    "- Previous knowledge (x3)\n",
    "- Name (x4)\n",
    "\n",
    "This means that our neural network will take four inputs: `x1`, `x2`, `x3`, and `x4`. The output will be a single value, which is either `0` or `1` (pass or fail).\n",
    "\n",
    "As you can probably guess, not all of these features are useful for predicting the outcome. For example, your name is not a good predictor of whether you will pass the exam or not.\n",
    "\n",
    "Let's walk through the steps of how the neural network will work.\n",
    "\n",
    "> Note: I will include videos and articles that explain the concepts in more detail in the last section of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008e826",
   "metadata": {},
   "source": [
    "## Neural Network Structure\n",
    "\n",
    "A neural network is made up of layers of neurons. Each neuron takes inputs, applies a transformation, and produces an output. The output of one layer becomes the input to the next layer.\n",
    "\n",
    "To take our previous example, we can represent the neural network structure as follows:\n",
    "\n",
    "<img src=\"../09_images/01-neural_network_structure.png\" alt=\"Neural Network Structure\" width=\"800\">\n",
    "\n",
    "The neural network has:\n",
    "\n",
    "- **Input Layer**: The first layer that takes the inputs `x1`, `x2`, `x3`, and `x4`.\n",
    "- **Hidden Layer**: The layer that processes the inputs and applies transformations. In this case, we have one neuron in the hidden layer, but we could have more neurons and more layers for a more complex model.\n",
    "- **Output Layer**: The final layer that produces the output. In this case, we have one output neuron that gives the final prediction.\n",
    "\n",
    "### Concrete Example\n",
    "\n",
    "Let's consider a simple example to make this more tangible. Imagine we have a neural network with:\n",
    "\n",
    "- 2 input features: Hours studied (x₁) and Previous knowledge (x₂)\n",
    "- 1 hidden layer with 2 neurons\n",
    "- 1 output neuron predicting exam pass/fail probability\n",
    "\n",
    "For this example:\n",
    "\n",
    "- Input values: x₁ = 3 (hours studied), x₂ = 7 (previous knowledge on a scale of 1-10)\n",
    "- Weights from inputs to first hidden neuron: w₁₁ = 0.2, w₁₂ = 0.3\n",
    "- Weights from inputs to second hidden neuron: w₂₁ = 0.1, w₂₂ = 0.4\n",
    "- Bias for first hidden neuron: b₁ = 0.5\n",
    "- Bias for second hidden neuron: b₂ = 0.1\n",
    "- Weights from hidden neurons to output: w₃₁ = 0.6, w₃₂ = 0.8\n",
    "- Bias for output neuron: b₃ = 0.2\n",
    "- We'll use the sigmoid activation function: σ(x) = 1/(1 + e^(-x))\n",
    "\n",
    "Step-by-step calculation:\n",
    "\n",
    "1. **First hidden neuron:**\n",
    "\n",
    "   - Weighted sum: (0.2 × 3) + (0.3 × 7) + 0.5 = 0.6 + 2.1 + 0.5 = 3.2\n",
    "   - Apply activation: σ(3.2) ≈ 0.961\n",
    "\n",
    "2. **Second hidden neuron:**\n",
    "\n",
    "   - Weighted sum: (0.1 × 3) + (0.4 × 7) + 0.1 = 0.3 + 2.8 + 0.1 = 3.2\n",
    "   - Apply activation: σ(3.2) ≈ 0.961\n",
    "\n",
    "3. **Output neuron:**\n",
    "   - Weighted sum: (0.6 × 0.961) + (0.8 × 0.961) + 0.2 = 0.577 + 0.769 + 0.2 = 1.546\n",
    "   - Apply activation: σ(1.546) ≈ 0.824\n",
    "\n",
    "So the neural network predicts a 82.4% probability of passing the exam with 3 hours of studying and a previous knowledge level of 7/10.\n",
    "\n",
    "This example illustrates how information flows forward through the network, from inputs through hidden layers to the output.\n",
    "\n",
    "Now let's investigate how the neural network works step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c9802",
   "metadata": {},
   "source": [
    "### Step 1: Initializating Weights and Biases\n",
    "\n",
    "In a neural network, we have **[weights](https://www.geeksforgeeks.org/deep-learning/the-role-of-weights-and-bias-in-neural-networks/)** and **[biases](https://www.turing.com/kb/necessity-of-bias-in-neural-networks)**. Weights are the parameters that the model learns during training, and biases are added to the weighted sum of inputs to help the model fit the data better.\n",
    "\n",
    "The weights and biases are initialized randomly at the beginning of the training process. For our example, we will have four weights (one for each input) and one bias.\n",
    "\n",
    "The weights measure the importance of each input feature, while the bias allows the model to shift the output up or down. As mentioned earlier, not all features are useful for predicting the outcome, so the weights will adjust accordingly during training.\n",
    "\n",
    "For example, if the weight for `x4` (name) is close to zero, it means that the name is not a useful feature for predicting the outcome. The model will learn to ignore it.\n",
    "\n",
    "### Step 2: Forward Pass\n",
    "\n",
    "In the forward pass, the inputs are multiplied by their corresponding weights, and the bias is added to the weighted sum. This is done for each neuron in the hidden layer.\n",
    "\n",
    "The output of the hidden layer is then passed through an activation function, which introduces non-linearity into the model. This allows the neural network to learn complex patterns in the data (this will be explained in more detail in a later section).\n",
    "\n",
    "<img src=\"../09_images/01-weight_initialization.png\" alt=\"Initializing Weights and Biases\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d50b45",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "\n",
    "Activation functions are a crucial component of neural networks. They determine whether a neuron should be activated or not by calculating a weighted sum and adding bias. Without activation functions, neural networks would only be capable of learning linear relationships, which would severely limit their capabilities.\n",
    "\n",
    "Some common activation functions include:\n",
    "\n",
    "- **Sigmoid**: Maps any input value to a value between 0 and 1. It's useful for binary classification problems but suffers from the vanishing gradient problem.\n",
    "  - Formula: σ(x) = 1 / (1 + e^(-x))\n",
    "- **ReLU (Rectified Linear Unit)**: Outputs the input directly if it's positive, otherwise, it outputs zero. It's the most commonly used activation function because it's computationally efficient and helps mitigate the vanishing gradient problem.\n",
    "  - Formula: f(x) = max(0, x)\n",
    "- **Tanh (Hyperbolic Tangent)**: Similar to sigmoid but maps values between -1 and 1. It has stronger gradients than sigmoid.\n",
    "  - Formula: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "- **Leaky ReLU**: A variant of ReLU that allows a small, non-zero gradient when the input is negative.\n",
    "  - Formula: f(x) = x if x > 0, αx otherwise (where α is a small constant)\n",
    "- **Softmax**: Often used in the output layer for multi-class classification problems, it converts a vector of values to a probability distribution.\n",
    "  - Formula: softmax(x_i) = e^(x_i) / Σ e^(x_j) for all j\n",
    "\n",
    "The choice of activation function depends on the specific task and layer of the neural network. For example, ReLU is commonly used in hidden layers, while sigmoid or softmax might be used in the output layer depending on the problem type.\n",
    "\n",
    "<img src=\"../09_images/01-weight_initialization.png\" alt=\"Initializing Weights and Biases\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921a0cd",
   "metadata": {},
   "source": [
    "### Step 3: Calculating Loss\n",
    "\n",
    "After the forward pass, we need to calculate the loss, which measures how well the model's predictions match the actual labels. The loss function quantifies the difference between the predicted output and the true output.\n",
    "\n",
    "### Step 4: Backward Pass\n",
    "\n",
    "In the backward pass, we calculate the gradients of the loss with respect to the weights and biases. This is done using **backpropagation**, which is a method for calculating the gradients efficiently.\n",
    "\n",
    "The gradients tell us how much the loss will change if we adjust the weights and biases. We use these gradients to update the weights and biases in the direction that reduces the loss.\n",
    "\n",
    "### Step 5: Updating Weights and Biases\n",
    "\n",
    "After calculating the gradients, we update the weights and biases using an optimization algorithm. The weights and biases are adjusted in the direction that minimizes the loss.\n",
    "This process is repeated for multiple iterations (epochs) until the model converges and the loss reaches an acceptable level.\n",
    "\n",
    "To summarize, the steps of a neural network are:\n",
    "\n",
    "1. Initialize weights and biases randomly.\n",
    "2. Perform a forward pass to calculate the output.\n",
    "3. Calculate the loss.\n",
    "4. Perform a backward pass to calculate gradients.\n",
    "5. Update weights and biases using the gradients.\n",
    "6. Repeat steps 2-5 for multiple epochs until convergence.\n",
    "\n",
    "<img src=\"../09_images/01-neural_network_complete.png\" alt=\"Neural Network Steps\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96f4ca",
   "metadata": {},
   "source": [
    "## Types of Neural Networks\n",
    "\n",
    "Neural networks come in various architectures, each designed for specific types of problems. Here are some common types:\n",
    "\n",
    "### Feedforward Neural Networks (FNN)\n",
    "\n",
    "- The most basic type where information flows in one direction from input to output\n",
    "- No cycles or loops in the network\n",
    "- Used for classification and regression problems\n",
    "- Example: The network we've been discussing so far is a simple feedforward network\n",
    "\n",
    "### Convolutional Neural Networks (CNN)\n",
    "\n",
    "- Specialized for processing grid-like data such as images\n",
    "- Uses convolutional layers to extract features\n",
    "- Employs pooling layers to reduce dimensionality\n",
    "- Excellent for image classification, object detection, and computer vision tasks\n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "\n",
    "- Designed for sequential data where order matters\n",
    "- Contains loops allowing information persistence\n",
    "- Can process inputs of variable length\n",
    "- Used for natural language processing, time series prediction, and speech recognition\n",
    "- Variants include LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) networks\n",
    "\n",
    "### Generative Adversarial Networks (GAN)\n",
    "\n",
    "- Consists of two networks: a generator and a discriminator competing against each other\n",
    "- Generator creates synthetic data, discriminator evaluates its authenticity\n",
    "- Used for generating realistic images, music, and text\n",
    "\n",
    "### Transformer Networks\n",
    "\n",
    "- Based on attention mechanisms rather than recurrence\n",
    "- Processes all input data in parallel\n",
    "- Revolutionized NLP tasks\n",
    "- Examples include BERT, GPT models, and other large language models\n",
    "\n",
    "### Self-Organizing Maps (SOM)\n",
    "\n",
    "- Unsupervised learning networks that produce low-dimensional representations of input data\n",
    "- Preserves topological properties of input space\n",
    "- Used for dimensionality reduction and data visualization\n",
    "\n",
    "Each of these architectures has its strengths and weaknesses, and the choice of network depends on the specific problem you're trying to solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0b2b3",
   "metadata": {},
   "source": [
    "## Extra Reading\n",
    "\n",
    "For a good overview of neural networks, I would highly recommend going through these 4 videos by 3Blue1Brown:\n",
    "\n",
    "1. [But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "2. [Gradient Descent, How Neural Networks Learn](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "3. [Backpropagation, Intuitively](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "4. [Backpropagation, Calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "\n",
    "For loss functions, I would recommend reading the following articles:\n",
    "\n",
    "- [Loss Functions Explained](https://medium.com/deep-learning-demystified/loss-functions-explained-3098e8ff2b27)\n",
    "- [PyTorch Loss Functions: The Ultimate Guide](https://neptune.ai/blog/pytorch-loss-functions)\n",
    "- [Understanding Loss Functions in Deep Learning](https://towardsdatascience.com/understanding-loss-functions-in-deep-learning-for-effective-model-training-5de13424c7d2)\n",
    "- [Choosing the Right Loss Function for Your Neural Network](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "\n",
    "For gradient descent, I would recommend reading the following articles:\n",
    "\n",
    "- [Understanding Gradient Descent and Its Variants](https://towardsdatascience.com/understanding-gradient-descent-and-its-variants-1e5a3596ec3a)\n",
    "- [Gradient Descent Algorithm and Its Variants](https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/)\n",
    "- [Gradient Descent Optimization Algorithms Explained](https://ruder.io/optimizing-gradient-descent/)\n",
    "- [Visualizing and Understanding Gradient Descent](https://distill.pub/2017/momentum/)\n",
    "\n",
    "For backpropagation, I would recommend reading the following articles:\n",
    "\n",
    "- [Backpropagation in Neural Networks: A Step-by-Step Guide](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)\n",
    "- [Mathematics of Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)\n",
    "- [Backpropagation - Visual and Interactive Explanation](https://developers.google.com/machine-learning/crash-course/backprop-scroll)\n",
    "\n",
    "For activation functions:\n",
    "\n",
    "- [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)\n",
    "- [Activation Functions in Neural Networks: A Comprehensive Guide](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    "- [Visualizing Activation Functions](https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d8493",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
