{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78a3a427",
      "metadata": {},
      "source": [
        "# 07. Gradients and Autograd - The Engine of Deep Learning ⚡\n",
        "\n",
        "Welcome to the **most fundamental** concept in deep learning! 🧠\n",
        "\n",
        "Understanding gradients and automatic differentiation is like understanding the engine of a car - you don't need to know every detail to drive, but knowing how it works makes you a much better driver!\n",
        "\n",
        "## 🎯 Learning Objectives\n",
        "\n",
        "Master the core engine that powers all deep learning:\n",
        "\n",
        "- **What gradients are** and why they're crucial for learning\n",
        "- **How automatic differentiation works** step by step\n",
        "- **PyTorch's computational graph** and how to work with it\n",
        "- **Advanced gradient techniques** used in production\n",
        "- **Common gradient problems** and how to solve them\n",
        "- **Custom gradient functions** for advanced use cases\n",
        "\n",
        "## 🧩 What You'll Build\n",
        "\n",
        "- **Interactive gradient visualizer** - See gradients in action!\n",
        "- **Computational graph explorer** - Understand how PyTorch tracks operations\n",
        "- **Gradient debugging toolkit** - Diagnose and fix gradient problems\n",
        "- **Custom autograd functions** - Extend PyTorch's capabilities\n",
        "- **Performance optimization** - Make gradient computation faster\n",
        "\n",
        "## 🌟 Why This Matters\n",
        "\n",
        "Gradients are the **heartbeat** of machine learning:\n",
        "\n",
        "| Without Understanding Gradients | With Gradient Mastery         |\n",
        "| ------------------------------- | ----------------------------- |\n",
        "| 🎲 Trial and error debugging    | 🔍 Systematic problem solving |\n",
        "| 🐌 Inefficient training         | ⚡ Optimized performance      |\n",
        "| 🚫 Limited to basic models      | 🚀 Advanced architectures     |\n",
        "| 🤷‍♂️ \"Black box\" thinking         | 🧠 Deep understanding         |\n",
        "\n",
        "Let's unlock the secrets of automatic differentiation! 🔓\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e8767e",
      "metadata": {},
      "source": [
        "## 🧮 What Are Gradients and Why Do We Need Them?\n",
        "\n",
        "Before diving into automatic differentiation, let's understand what gradients actually are and why they're the foundation of all machine learning!\n",
        "\n",
        "### 🎯 Gradients: The GPS of Machine Learning\n",
        "\n",
        "Think of gradients as **directions on a map**:\n",
        "\n",
        "- 🗺️ **The map**: The loss landscape (how wrong our model is)\n",
        "- 📍 **Current location**: Current model parameters\n",
        "- 🧭 **Gradient**: Which direction to move to reduce error\n",
        "- 🚶‍♂️ **Step size**: Learning rate (how big steps to take)\n",
        "\n",
        "### 📊 Mathematical Intuition\n",
        "\n",
        "A gradient tells us:\n",
        "\n",
        "1. **Direction**: Which way to adjust parameters\n",
        "2. **Magnitude**: How much the loss changes with small parameter changes\n",
        "\n",
        "For a function f(x), the gradient df/dx tells us:\n",
        "\n",
        "- If **positive**: Increasing x increases f(x)\n",
        "- If **negative**: Increasing x decreases f(x)\n",
        "- If **zero**: We're at a local minimum/maximum\n",
        "\n",
        "### 🤖 Why Automatic Differentiation?\n",
        "\n",
        "Computing gradients manually for complex neural networks would be:\n",
        "\n",
        "- ⏰ **Extremely time-consuming** (thousands of parameters!)\n",
        "- 🐛 **Error-prone** (one mistake ruins everything)\n",
        "- 🔄 **Inflexible** (change model = recompute everything)\n",
        "\n",
        "**Automatic differentiation** solves this by computing exact gradients automatically! ✨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837f452f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.7.1\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set style for professional plots\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"🚀 Gradient and AutoGrad Learning Environment Ready!\")\n",
        "print(\"📚 Let's explore the magic behind automatic differentiation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a9ff92",
      "metadata": {},
      "source": [
        "## 🎨 Visualizing Gradients: From Simple to Complex\n",
        "\n",
        "Let's start with visual intuition before diving into code!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e88ee93d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x = 2.0\n",
            "x.requires_grad: True\n",
            "y = 11.0\n",
            "y.grad_fn: <AddBackward0 object at 0x107afc370>\n",
            "\n",
            "Computational graph:\n",
            "x -> None\n",
            "y -> <AddBackward0 object at 0x107bf5a50>\n"
          ]
        }
      ],
      "source": [
        "def visualize_1d_gradient():\n",
        "    \"\"\"Visualize gradient as slope on a 1D function\"\"\"\n",
        "\n",
        "    # Create a simple quadratic function\n",
        "    x = np.linspace(-3, 3, 100)\n",
        "    y = x**2 + 2 * x + 1  # f(x) = x² + 2x + 1\n",
        "\n",
        "    # Analytical gradient: f'(x) = 2x + 2\n",
        "    gradient = 2 * x + 2\n",
        "\n",
        "    # Pick a point to show tangent\n",
        "    point_x = 1.5\n",
        "    point_y = point_x**2 + 2 * point_x + 1\n",
        "    point_grad = 2 * point_x + 2\n",
        "\n",
        "    # Create tangent line\n",
        "    tangent_x = np.linspace(point_x - 0.8, point_x + 0.8, 10)\n",
        "    tangent_y = point_y + point_grad * (tangent_x - point_x)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Plot 1: Function with tangent line\n",
        "    ax1.plot(x, y, \"b-\", linewidth=3, label=\"f(x) = x² + 2x + 1\")\n",
        "    ax1.plot(\n",
        "        point_x,\n",
        "        point_y,\n",
        "        \"ro\",\n",
        "        markersize=10,\n",
        "        label=f\"Point ({point_x:.1f}, {point_y:.1f})\",\n",
        "    )\n",
        "    ax1.plot(\n",
        "        tangent_x,\n",
        "        tangent_y,\n",
        "        \"r--\",\n",
        "        linewidth=2,\n",
        "        label=f\"Tangent (slope = {point_grad:.1f})\",\n",
        "    )\n",
        "\n",
        "    # Add arrow showing gradient direction\n",
        "    arrow_scale = 0.3\n",
        "    ax1.arrow(\n",
        "        point_x,\n",
        "        point_y,\n",
        "        arrow_scale,\n",
        "        point_grad * arrow_scale,\n",
        "        head_width=0.1,\n",
        "        head_length=0.1,\n",
        "        fc=\"red\",\n",
        "        ec=\"red\",\n",
        "    )\n",
        "\n",
        "    ax1.set_xlabel(\"x\", fontsize=12)\n",
        "    ax1.set_ylabel(\"f(x)\", fontsize=12)\n",
        "    ax1.set_title(\n",
        "        \"🔍 Gradient as Slope of Tangent Line\", fontsize=14, fontweight=\"bold\"\n",
        "    )\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Gradient function\n",
        "    ax2.plot(x, gradient, \"g-\", linewidth=3, label=\"f'(x) = 2x + 2\")\n",
        "    ax2.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\n",
        "    ax2.axvline(\n",
        "        x=-1, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Zero gradient (x = -1)\"\n",
        "    )\n",
        "    ax2.plot(\n",
        "        point_x, point_grad, \"ro\", markersize=10, label=f\"Gradient at x={point_x:.1f}\"\n",
        "    )\n",
        "\n",
        "    ax2.set_xlabel(\"x\", fontsize=12)\n",
        "    ax2.set_ylabel(\"f'(x)\", fontsize=12)\n",
        "    ax2.set_title(\"📈 Gradient Function\", fontsize=14, fontweight=\"bold\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"🎯 At x = {point_x}:\")\n",
        "    print(f\"   📊 Function value: f({point_x}) = {point_y:.2f}\")\n",
        "    print(f\"   📐 Gradient: f'({point_x}) = {point_grad:.2f}\")\n",
        "    print(\n",
        "        f\"   🧭 Direction: {'Increasing' if point_grad > 0 else 'Decreasing' if point_grad < 0 else 'Flat'}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Visualize 1D gradients\n",
        "visualize_1d_gradient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bca4d64",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_2d_gradient():\n",
        "    \"\"\"Visualize gradients in 2D with contour plots and vector fields\"\"\"\n",
        "\n",
        "    # Create a 2D function: f(x,y) = x² + y² (bowl shape)\n",
        "    x = np.linspace(-3, 3, 30)\n",
        "    y = np.linspace(-3, 3, 30)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = X**2 + Y**2\n",
        "\n",
        "    # Compute gradients: ∇f = [2x, 2y]\n",
        "    dZ_dx = 2 * X\n",
        "    dZ_dy = 2 * Y\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Plot 1: 3D surface\n",
        "    ax1 = fig.add_subplot(131, projection=\"3d\")\n",
        "    surf = ax1.plot_surface(X, Y, Z, cmap=\"viridis\", alpha=0.7)\n",
        "    ax1.set_xlabel(\"x\")\n",
        "    ax1.set_ylabel(\"y\")\n",
        "    ax1.set_zlabel(\"f(x,y)\")\n",
        "    ax1.set_title(\"🏔️ 3D Function Surface\\nf(x,y) = x² + y²\", fontweight=\"bold\")\n",
        "\n",
        "    # Plot 2: Contour with gradient vectors\n",
        "    ax2 = fig.add_subplot(132)\n",
        "    contour = ax2.contour(X, Y, Z, levels=15, colors=\"blue\", alpha=0.6)\n",
        "    ax2.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "    # Show gradient vectors (downsampled for clarity)\n",
        "    skip = 3\n",
        "    ax2.quiver(\n",
        "        X[::skip, ::skip],\n",
        "        Y[::skip, ::skip],\n",
        "        -dZ_dx[::skip, ::skip],\n",
        "        -dZ_dy[::skip, ::skip],  # Negative for descent direction\n",
        "        color=\"red\",\n",
        "        alpha=0.7,\n",
        "        scale=50,\n",
        "    )\n",
        "\n",
        "    ax2.set_xlabel(\"x\")\n",
        "    ax2.set_ylabel(\"y\")\n",
        "    ax2.set_title(\n",
        "        \"🗺️ Contour + Gradient Vectors\\n(Red arrows = descent direction)\",\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "    ax2.set_aspect(\"equal\")\n",
        "\n",
        "    # Plot 3: Gradient magnitude\n",
        "    ax3 = fig.add_subplot(133)\n",
        "    gradient_magnitude = np.sqrt(dZ_dx**2 + dZ_dy**2)\n",
        "    im = ax3.imshow(\n",
        "        gradient_magnitude, extent=[-3, 3, -3, 3], origin=\"lower\", cmap=\"hot\"\n",
        "    )\n",
        "    ax3.set_xlabel(\"x\")\n",
        "    ax3.set_ylabel(\"y\")\n",
        "    ax3.set_title(\n",
        "        \"🔥 Gradient Magnitude\\n|∇f| = √((∂f/∂x)² + (∂f/∂y)²)\", fontweight=\"bold\"\n",
        "    )\n",
        "    plt.colorbar(im, ax=ax3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"🎯 Key Insights:\")\n",
        "    print(\"   🏔️ Function surface shows the 'landscape' our model needs to navigate\")\n",
        "    print(\"   🧭 Red arrows point in steepest DESCENT direction (where we want to go!)\")\n",
        "    print(\"   🔥 Gradient magnitude shows how 'steep' the slope is at each point\")\n",
        "    print(\"   🎯 At the center (0,0), gradient = [0,0] → we found the minimum!\")\n",
        "\n",
        "\n",
        "# Visualize 2D gradients\n",
        "visualize_2d_gradient()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4fcab8",
      "metadata": {},
      "source": [
        "## 🔄 Computational Graphs: PyTorch's Secret Weapon\n",
        "\n",
        "PyTorch builds a **computational graph** behind the scenes to track operations and compute gradients automatically!\n",
        "\n",
        "### 🏗️ How It Works:\n",
        "\n",
        "1. **Forward Pass**: Build the graph while computing function values\n",
        "2. **Backward Pass**: Traverse graph in reverse to compute gradients\n",
        "3. **Chain Rule**: Automatically applies chain rule at each node\n",
        "\n",
        "Let's see this in action! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235e1c4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_computational_graph():\n",
        "    \"\"\"Show how PyTorch builds computational graphs\"\"\"\n",
        "\n",
        "    print(\"🏗️ Building a Simple Computational Graph\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create tensors with gradient tracking\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "    y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "    print(f\"📊 Input tensors:\")\n",
        "    print(f\"   x = {x.item():.2f} (requires_grad={x.requires_grad})\")\n",
        "    print(f\"   y = {y.item():.2f} (requires_grad={y.requires_grad})\")\n",
        "\n",
        "    # Build computational graph step by step\n",
        "    print(f\"\\n🔄 Forward pass (building the graph):\")\n",
        "\n",
        "    # Step 1: z = x * y\n",
        "    z = x * y\n",
        "    print(f\"   Step 1: z = x * y = {z.item():.2f}\")\n",
        "    print(f\"           Graph node: {z.grad_fn}\")\n",
        "\n",
        "    # Step 2: w = z + x\n",
        "    w = z + x\n",
        "    print(f\"   Step 2: w = z + x = {w.item():.2f}\")\n",
        "    print(f\"           Graph node: {w.grad_fn}\")\n",
        "\n",
        "    # Step 3: loss = w²\n",
        "    loss = w**2\n",
        "    print(f\"   Step 3: loss = w² = {loss.item():.2f}\")\n",
        "    print(f\"           Graph node: {loss.grad_fn}\")\n",
        "\n",
        "    # Backward pass\n",
        "    print(f\"\\n⬅️ Backward pass (computing gradients):\")\n",
        "    loss.backward()\n",
        "\n",
        "    print(f\"   ∂loss/∂x = {x.grad.item():.2f}\")\n",
        "    print(f\"   ∂loss/∂y = {y.grad.item():.2f}\")\n",
        "\n",
        "    # Manual verification\n",
        "    print(f\"\\n🧮 Manual verification:\")\n",
        "    print(f\"   loss = (x*y + x)² = (x*(y+1))²\")\n",
        "    print(\n",
        "        f\"   ∂loss/∂x = 2*x*(y+1)*(y+1) = 2*x*(y+1)² = 2*{x.item()}*{(y.item()+1)**2} = {2*x.item()*(y.item()+1)**2:.2f} ✅\"\n",
        "    )\n",
        "    print(\n",
        "        f\"   ∂loss/∂y = 2*x*(y+1)*x = 2*x²*(y+1) = 2*{x.item()**2}*{(y.item()+1)} = {2*x.item()**2*(y.item()+1):.2f} ✅\"\n",
        "    )\n",
        "\n",
        "\n",
        "demonstrate_computational_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f73c3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1: x.grad = 4.0\n",
            "Iteration 2: x.grad = 9.0\n",
            "Iteration 3: x.grad = 15.0\n",
            "After reset: x.grad = None\n"
          ]
        }
      ],
      "source": [
        "def gradient_accumulation_demo():\n",
        "    \"\"\"Demonstrate gradient accumulation behavior\"\"\"\n",
        "\n",
        "    print(\"📈 Gradient Accumulation Demo\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "    print(f\"📊 Initial state:\")\n",
        "    print(f\"   x = {x.item()}, x.grad = {x.grad}\")\n",
        "\n",
        "    print(f\"\\n🔄 Multiple backward passes:\")\n",
        "\n",
        "    # First backward pass\n",
        "    y1 = x**2  # dy1/dx = 2x\n",
        "    y1.backward()\n",
        "    print(f\"   After y₁ = x²: x.grad = {x.grad.item():.2f}\")\n",
        "\n",
        "    # Second backward pass (accumulates!)\n",
        "    y2 = 3 * x  # dy2/dx = 3\n",
        "    y2.backward()\n",
        "    print(f\"   After y₂ = 3x: x.grad = {x.grad.item():.2f} (accumulated!)\")\n",
        "\n",
        "    # Third backward pass\n",
        "    y3 = x + 1  # dy3/dx = 1\n",
        "    y3.backward()\n",
        "    print(f\"   After y₃ = x+1: x.grad = {x.grad.item():.2f} (accumulated!)\")\n",
        "\n",
        "    print(\n",
        "        f\"\\n🧮 Expected: 2x + 3 + 1 = 2({x.item()}) + 3 + 1 = {2*x.item() + 3 + 1:.2f} ✅\"\n",
        "    )\n",
        "\n",
        "    # Reset gradients\n",
        "    print(f\"\\n🔄 Resetting gradients:\")\n",
        "    x.grad.zero_()  # or x.grad = None\n",
        "    print(f\"   After reset: x.grad = {x.grad}\")\n",
        "\n",
        "\n",
        "gradient_accumulation_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9437d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimization_visualization():\n",
        "    \"\"\"Visualize how gradients guide optimization\"\"\"\n",
        "\n",
        "    print(\"🎯 Gradient-Based Optimization Visualization\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Define a 2D function to optimize\n",
        "    def func(x, y):\n",
        "        return (x - 2) ** 2 + (y - 1) ** 2 + 0.5 * torch.sin(5 * x) * torch.cos(5 * y)\n",
        "\n",
        "    # Starting point\n",
        "    x = torch.tensor(-1.0, requires_grad=True)\n",
        "    y = torch.tensor(2.0, requires_grad=True)\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    # Track optimization path\n",
        "    path_x, path_y, losses = [], [], []\n",
        "\n",
        "    print(f\"🚀 Starting optimization from ({x.item():.2f}, {y.item():.2f})\")\n",
        "\n",
        "    for step in range(10):\n",
        "        # Zero gradients\n",
        "        if x.grad is not None:\n",
        "            x.grad.zero_()\n",
        "        if y.grad is not None:\n",
        "            y.grad.zero_()\n",
        "\n",
        "        # Forward pass\n",
        "        loss = func(x, y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Store current state\n",
        "        path_x.append(x.item())\n",
        "        path_y.append(y.item())\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        print(\n",
        "            f\"   Step {step:2d}: ({x.item():6.3f}, {y.item():6.3f}) → Loss: {loss.item():7.4f} → Grad: ({x.grad.item():6.3f}, {y.grad.item():6.3f})\"\n",
        "        )\n",
        "\n",
        "        # Update parameters\n",
        "        with torch.inference_mode():\n",
        "            x -= learning_rate * x.grad\n",
        "            y -= learning_rate * y.grad\n",
        "\n",
        "    print(f\"\\n🏁 Final position: ({x.item():.3f}, {y.item():.3f})\")\n",
        "    print(f\"🎯 Target (global minimum): (2.000, 1.000)\")\n",
        "\n",
        "    # Create visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Plot 1: Function contour with optimization path\n",
        "    x_range = np.linspace(-2, 3, 50)\n",
        "    y_range = np.linspace(-1, 3, 50)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "\n",
        "    # Compute function values\n",
        "    Z = np.zeros_like(X)\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            Z[i, j] = func(torch.tensor(X[i, j]), torch.tensor(Y[i, j])).item()\n",
        "\n",
        "    # Contour plot\n",
        "    contour = ax1.contour(X, Y, Z, levels=20, alpha=0.6)\n",
        "    ax1.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "    # Plot optimization path\n",
        "    ax1.plot(\n",
        "        path_x, path_y, \"ro-\", linewidth=2, markersize=6, label=\"Optimization Path\"\n",
        "    )\n",
        "    ax1.plot(path_x[0], path_y[0], \"go\", markersize=10, label=\"Start\")\n",
        "    ax1.plot(path_x[-1], path_y[-1], \"bs\", markersize=10, label=\"End\")\n",
        "    ax1.plot(2, 1, \"r*\", markersize=15, label=\"Global Minimum\")\n",
        "\n",
        "    ax1.set_xlabel(\"x\")\n",
        "    ax1.set_ylabel(\"y\")\n",
        "    ax1.set_title(\"🗺️ Optimization Path on Loss Landscape\", fontweight=\"bold\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Loss over time\n",
        "    ax2.plot(range(len(losses)), losses, \"b-o\", linewidth=2, markersize=6)\n",
        "    ax2.set_xlabel(\"Optimization Step\")\n",
        "    ax2.set_ylabel(\"Loss Value\")\n",
        "    ax2.set_title(\"📉 Loss Reduction Over Time\", fontweight=\"bold\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n🎓 Key Insights:\")\n",
        "    print(f\"   📐 Gradients point in direction of steepest ASCENT\")\n",
        "    print(f\"   ⬇️ We move in OPPOSITE direction to minimize loss\")\n",
        "    print(f\"   📏 Learning rate controls step size\")\n",
        "    print(f\"   🎯 Gradients get smaller as we approach minimum\")\n",
        "\n",
        "\n",
        "optimization_visualization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf347a2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z = 46.0\n",
            "dz/dx = tensor([ 6.,  9., 12.])\n",
            "dz/dy = tensor([1., 2., 3.])\n",
            "Manual dz/dx: tensor([ 6.,  9., 12.])\n"
          ]
        }
      ],
      "source": [
        "def tensor_gradients_demo():\n",
        "    \"\"\"Demonstrate gradients with multi-dimensional tensors\"\"\"\n",
        "\n",
        "    print(\"🔢 Multi-Dimensional Tensor Gradients\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    # Create multi-dimensional tensors\n",
        "    x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "    y = torch.tensor([[2.0, 1.0], [1.0, 2.0]], requires_grad=True)\n",
        "\n",
        "    print(f\"📊 Input tensors:\")\n",
        "    print(f\"   x = \\n{x}\")\n",
        "    print(f\"   y = \\n{y}\")\n",
        "\n",
        "    # Compute element-wise operations\n",
        "    z = x * y  # Element-wise multiplication\n",
        "    w = z.sum()  # Sum all elements to get scalar\n",
        "\n",
        "    print(f\"\\n🔄 Operations:\")\n",
        "    print(f\"   z = x ⊙ y = \\n{z}\")\n",
        "    print(f\"   w = sum(z) = {w.item():.2f}\")\n",
        "\n",
        "    # Compute gradients\n",
        "    w.backward()\n",
        "\n",
        "    print(f\"\\n⬅️ Gradients:\")\n",
        "    print(f\"   ∂w/∂x = \\n{x.grad}\")\n",
        "    print(f\"   ∂w/∂y = \\n{y.grad}\")\n",
        "\n",
        "    print(f\"\\n🧮 Manual verification:\")\n",
        "    print(f\"   Since w = sum(x ⊙ y), we have:\")\n",
        "    print(f\"   ∂w/∂x[i,j] = y[i,j] and ∂w/∂y[i,j] = x[i,j]\")\n",
        "    print(f\"   Expected ∂w/∂x = y = \\n{y}\")\n",
        "    print(f\"   Expected ∂w/∂y = x = \\n{x}\")\n",
        "\n",
        "\n",
        "tensor_gradients_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa6a5a44",
      "metadata": {},
      "source": [
        "## Understanding requires_grad\n",
        "\n",
        "The `requires_grad` parameter controls whether PyTorch should track operations for gradient computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d8914f1a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a.requires_grad: True\n",
            "b.requires_grad: False\n",
            "c.requires_grad: False\n",
            "(a * b).requires_grad: True\n",
            "(b * c).requires_grad: False\n"
          ]
        }
      ],
      "source": [
        "# Tensors with and without gradient tracking\n",
        "a = torch.tensor(2.0, requires_grad=True)\n",
        "b = torch.tensor(3.0, requires_grad=False)\n",
        "c = torch.tensor(4.0)  # requires_grad=False by default\n",
        "\n",
        "print(f\"a.requires_grad: {a.requires_grad}\")\n",
        "print(f\"b.requires_grad: {b.requires_grad}\")\n",
        "print(f\"c.requires_grad: {c.requires_grad}\")\n",
        "\n",
        "# Operations inherit gradient requirements\n",
        "result1 = a * b  # Result requires grad because a does\n",
        "result2 = b * c  # Result doesn't require grad because neither b nor c do\n",
        "\n",
        "print(f\"(a * b).requires_grad: {result1.requires_grad}\")\n",
        "print(f\"(b * c).requires_grad: {result2.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e64f6eed",
      "metadata": {},
      "source": [
        "## Detaching from the Computational Graph\n",
        "\n",
        "Sometimes you want to stop gradient computation for certain operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c4d976b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y1.requires_grad: True\n",
            "y2.requires_grad: False\n",
            "y3.requires_grad: False\n",
            "x.data_ptr() == x.detach().data_ptr(): True\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Normal computation\n",
        "y1 = x**2\n",
        "print(f\"y1.requires_grad: {y1.requires_grad}\")\n",
        "\n",
        "# Detached computation\n",
        "y2 = x.detach() ** 2\n",
        "print(f\"y2.requires_grad: {y2.requires_grad}\")\n",
        "\n",
        "# Using torch.inference_mode() (preferred over torch.inference_mode())\n",
        "with torch.inference_mode():\n",
        "    y3 = x**2\n",
        "    print(f\"y3.requires_grad: {y3.requires_grad}\")\n",
        "\n",
        "# The detached tensor shares storage but not gradient history\n",
        "print(f\"x.data_ptr() == x.detach().data_ptr(): {x.data_ptr() == x.detach().data_ptr()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e7887c0",
      "metadata": {},
      "source": [
        "## Neural Network Example: Manual Implementation\n",
        "\n",
        "Let's implement a simple neural network manually to see autograd in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c27c1419",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 1.1897\n",
            "Epoch 20, Loss: 0.7134\n",
            "Epoch 40, Loss: 0.5733\n",
            "Epoch 60, Loss: 0.4824\n",
            "Epoch 80, Loss: 0.4119\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a simple dataset\n",
        "torch.manual_seed(42)\n",
        "X = torch.randn(100, 2)  # 100 samples, 2 features\n",
        "y = (X[:, 0] + X[:, 1] > 0).float().unsqueeze(1)  # Simple classification\n",
        "\n",
        "# Manual neural network parameters\n",
        "W1 = torch.randn(2, 5, requires_grad=True)  # Input to hidden\n",
        "b1 = torch.zeros(5, requires_grad=True)  # Hidden bias\n",
        "W2 = torch.randn(5, 1, requires_grad=True)  # Hidden to output\n",
        "b2 = torch.zeros(1, requires_grad=True)  # Output bias\n",
        "\n",
        "\n",
        "def forward(X):\n",
        "    \"\"\"Forward pass through the network\"\"\"\n",
        "    h = torch.relu(X @ W1 + b1)  # Hidden layer\n",
        "    out = torch.sigmoid(h @ W2 + b2)  # Output layer\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_loss(predictions, targets):\n",
        "    \"\"\"Binary cross-entropy loss\"\"\"\n",
        "    return -torch.mean(\n",
        "        targets * torch.log(predictions + 1e-8)\n",
        "        + (1 - targets) * torch.log(1 - predictions + 1e-8)\n",
        "    )\n",
        "\n",
        "\n",
        "# Training loop\n",
        "learning_rate = 0.1\n",
        "losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    # Forward pass\n",
        "    predictions = forward(X)\n",
        "    loss = compute_loss(predictions, y)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters manually\n",
        "    with torch.inference_mode():\n",
        "        W1 -= learning_rate * W1.grad\n",
        "        b1 -= learning_rate * b1.grad\n",
        "        W2 -= learning_rate * W2.grad\n",
        "        b2 -= learning_rate * b2.grad\n",
        "\n",
        "    # Clear gradients\n",
        "    W1.grad = None\n",
        "    b1.grad = None\n",
        "    W2.grad = None\n",
        "    b2.grad = None\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Plot the loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses)\n",
        "plt.title(\"Training Loss Over Time\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ad3a74",
      "metadata": {},
      "source": [
        "## Gradient Accumulation for Large Batches\n",
        "\n",
        "When memory is limited, you can simulate larger batch sizes by accumulating gradients across multiple smaller batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f41e3dc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 4.9993\n",
            "Epoch 2, Average Loss: 4.8793\n",
            "Epoch 3, Average Loss: 4.7709\n",
            "Epoch 4, Average Loss: 4.6728\n",
            "Epoch 5, Average Loss: 4.5839\n"
          ]
        }
      ],
      "source": [
        "# Simulate gradient accumulation\n",
        "model = nn.Linear(10, 1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Create some dummy data\n",
        "X = torch.randn(100, 10)\n",
        "y = torch.randn(100, 1)\n",
        "\n",
        "accumulation_steps = 4\n",
        "batch_size = 8\n",
        "\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()  # Clear gradients at the start of each epoch\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        # Get mini-batch\n",
        "        batch_X = X[i : i + batch_size]\n",
        "        batch_y = y[i : i + batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Scale loss by accumulation steps\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        # Backward pass (accumulates gradients)\n",
        "        loss.backward()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update parameters every accumulation_steps\n",
        "        if (i // batch_size + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "597aa8b9",
      "metadata": {},
      "source": [
        "## Custom Autograd Functions\n",
        "\n",
        "For advanced use cases, you can define custom autograd functions with custom forward and backward passes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e53e2e3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y = 9.0\n",
            "dy/dx = 6.0\n"
          ]
        }
      ],
      "source": [
        "class SquareFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Custom autograd function that squares its input.\n",
        "    This example shows how to define custom forward and backward passes.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        Forward pass: compute the output\n",
        "        ctx: context object to save information for backward pass\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)  # Save input for backward pass\n",
        "        return input * input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradient with respect to input\n",
        "        grad_output: gradient flowing back from the next layer\n",
        "        \"\"\"\n",
        "        (input,) = ctx.saved_tensors  # Retrieve saved input\n",
        "        grad_input = grad_output * 2 * input  # d/dx(x^2) = 2x\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "# Use the custom function\n",
        "square = SquareFunction.apply\n",
        "\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = square(x)\n",
        "print(f\"y = {y}\")\n",
        "\n",
        "y.backward()\n",
        "print(f\"dy/dx = {x.grad}\")  # Should be 2*3 = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ebfa2c",
      "metadata": {},
      "source": [
        "## 🛠️ Advanced AutoGrad Techniques\n",
        "\n",
        "Let's explore some advanced features that make PyTorch's autograd system so powerful!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e3fae6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def higher_order_gradients():\n",
        "    \"\"\"Demonstrate second-order gradients (gradients of gradients)\"\"\"\n",
        "\n",
        "    print(\"📈 Higher-Order Gradients Demo\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    # Create input with gradient tracking\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "    # Define function: f(x) = x⁴ + 2x³ + x²\n",
        "    y = x**4 + 2 * x**3 + x**2\n",
        "\n",
        "    print(f\"📊 Function: f(x) = x⁴ + 2x³ + x²\")\n",
        "    print(f\"   At x = {x.item()}: f(x) = {y.item():.2f}\")\n",
        "\n",
        "    # First derivative: f'(x) = 4x³ + 6x² + 2x\n",
        "    first_grad = torch.autograd.grad(y, x, create_graph=True)[0]\n",
        "    print(f\"\\n📐 First derivative: f'(x) = 4x³ + 6x² + 2x\")\n",
        "    print(f\"   f'({x.item()}) = {first_grad.item():.2f}\")\n",
        "\n",
        "    # Second derivative: f''(x) = 12x² + 12x + 2\n",
        "    second_grad = torch.autograd.grad(first_grad, x)[0]\n",
        "    print(f\"\\n📐📐 Second derivative: f''(x) = 12x² + 12x + 2\")\n",
        "    print(f\"   f''({x.item()}) = {second_grad.item():.2f}\")\n",
        "\n",
        "    # Manual verification\n",
        "    x_val = x.item()\n",
        "    manual_first = 4 * x_val**3 + 6 * x_val**2 + 2 * x_val\n",
        "    manual_second = 12 * x_val**2 + 12 * x_val + 2\n",
        "\n",
        "    print(f\"\\n🧮 Manual verification:\")\n",
        "    print(f\"   f'({x_val}) = {manual_first:.2f} ✅\")\n",
        "    print(f\"   f''({x_val}) = {manual_second:.2f} ✅\")\n",
        "\n",
        "    print(f\"\\n💡 Applications of higher-order gradients:\")\n",
        "    print(f\"   🎯 Newton's optimization method\")\n",
        "    print(f\"   🔍 Curvature analysis\")\n",
        "    print(f\"   🧠 Meta-learning algorithms\")\n",
        "\n",
        "\n",
        "higher_order_gradients()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d016b262",
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_autograd_function():\n",
        "    \"\"\"Demonstrate how to create custom autograd functions\"\"\"\n",
        "\n",
        "    print(\"\\n🔧 Custom AutoGrad Function\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    class MyReLU(torch.autograd.Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, input):\n",
        "            # Save input for backward pass\n",
        "            ctx.save_for_backward(input)\n",
        "            return input.clamp(min=0)\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            # Retrieve saved input\n",
        "            (input,) = ctx.saved_tensors\n",
        "            grad_input = grad_output.clone()\n",
        "            grad_input[input < 0] = 0  # Gradient is 0 where input < 0\n",
        "            return grad_input\n",
        "\n",
        "    # Use our custom function\n",
        "    x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
        "\n",
        "    print(f\"📊 Input: {x}\")\n",
        "\n",
        "    # Apply custom ReLU\n",
        "    y = MyReLU.apply(x)\n",
        "    print(f\"   Custom ReLU output: {y}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    loss = y.sum()\n",
        "    loss.backward()\n",
        "\n",
        "    print(f\"   Gradients: {x.grad}\")\n",
        "    print(f\"   Expected: [0, 0, 0, 1, 1] (gradient is 1 where input > 0, else 0)\")\n",
        "\n",
        "    print(f\"\\n💡 Custom functions are useful for:\")\n",
        "    print(f\"   ⚡ Implementing new operations\")\n",
        "    print(f\"   🔧 Optimizing performance\")\n",
        "    print(f\"   🎛️ Controlling gradient flow\")\n",
        "\n",
        "\n",
        "custom_autograd_function()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "860afc03",
      "metadata": {},
      "source": [
        "## 🐛 Debugging Gradients: Common Issues and Solutions\n",
        "\n",
        "Let's learn how to debug gradient-related problems!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08d2d20",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_debugging():\n",
        "    \"\"\"Demonstrate common gradient issues and how to debug them\"\"\"\n",
        "\n",
        "    print(\"🐛 Gradient Debugging Guide\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    # Issue 1: In-place operations\n",
        "    print(\"⚠️  Issue 1: In-place operations break gradients\")\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "    y = x**2\n",
        "\n",
        "    try:\n",
        "        y += 1  # This is in-place!\n",
        "        y.backward()\n",
        "    except RuntimeError as e:\n",
        "        print(f\"   Error: {str(e)[:60]}...\")\n",
        "        print(f\"   🔧 Solution: Use y = y + 1 instead of y += 1\")\n",
        "\n",
        "    # Issue 2: Missing requires_grad\n",
        "    print(f\"\\n⚠️  Issue 2: Missing requires_grad\")\n",
        "    x1 = torch.tensor(2.0)  # No requires_grad!\n",
        "    y1 = x1**2\n",
        "    print(f\"   x.requires_grad: {x1.requires_grad}\")\n",
        "    print(f\"   y.requires_grad: {y1.requires_grad}\")\n",
        "    print(f\"   🔧 Solution: Set requires_grad=True or use x.requires_grad_(True)\")\n",
        "\n",
        "    # Issue 3: Detached computation graph\n",
        "    print(f\"\\n⚠️  Issue 3: Accidentally detaching computation graph\")\n",
        "    x2 = torch.tensor(2.0, requires_grad=True)\n",
        "    y2 = x2**2\n",
        "    y2_detached = y2.detach()  # Breaks the graph!\n",
        "    z2 = y2_detached * 2\n",
        "    print(f\"   z.requires_grad: {z2.requires_grad}\")\n",
        "    print(f\"   🔧 Solution: Don't use .detach() unless intended\")\n",
        "\n",
        "    # Issue 4: Multiple backward passes without retain_graph\n",
        "    print(f\"\\n⚠️  Issue 4: Multiple backward passes\")\n",
        "    x3 = torch.tensor(2.0, requires_grad=True)\n",
        "    y3 = x3**2\n",
        "\n",
        "    y3.backward(retain_graph=True)  # First backward\n",
        "    print(f\"   First backward: x.grad = {x3.grad}\")\n",
        "\n",
        "    y3.backward()  # Second backward (possible with retain_graph=True)\n",
        "    print(f\"   Second backward: x.grad = {x3.grad} (accumulated!)\")\n",
        "\n",
        "    print(f\"\\n🔧 Debugging tools:\")\n",
        "    print(f\"   🔍 Check requires_grad property\")\n",
        "    print(f\"   🔍 Inspect grad_fn attribute\")\n",
        "    print(f\"   🔍 Use torch.autograd.grad() for more control\")\n",
        "    print(f\"   🔍 Use torch.autograd.gradcheck() for testing\")\n",
        "\n",
        "\n",
        "gradient_debugging()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e296d033",
      "metadata": {},
      "source": [
        "## 🎓 Summary and Best Practices\n",
        "\n",
        "Let's wrap up with key takeaways and best practices for using gradients effectively!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d001dac7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import FancyBboxPatch\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set style for professional plots\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"🚀 Gradient and AutoGrad Learning Environment Ready!\")\n",
        "print(\"📚 Let's explore the magic behind automatic differentiation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf9833b",
      "metadata": {},
      "source": [
        "## 🎨 Visualizing Gradients: From Simple to Complex\n",
        "\n",
        "Let's start with visual intuition before diving into code!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73489cf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_1d_gradient():\n",
        "    \"\"\"Visualize gradient as slope on a 1D function\"\"\"\n",
        "\n",
        "    # Create a simple quadratic function\n",
        "    x = np.linspace(-3, 3, 100)\n",
        "    y = x**2 + 2 * x + 1  # f(x) = x² + 2x + 1\n",
        "\n",
        "    # Analytical gradient: f'(x) = 2x + 2\n",
        "    gradient = 2 * x + 2\n",
        "\n",
        "    # Pick a point to show tangent\n",
        "    point_x = 1.5\n",
        "    point_y = point_x**2 + 2 * point_x + 1\n",
        "    point_grad = 2 * point_x + 2\n",
        "\n",
        "    # Create tangent line\n",
        "    tangent_x = np.linspace(point_x - 0.8, point_x + 0.8, 10)\n",
        "    tangent_y = point_y + point_grad * (tangent_x - point_x)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Plot 1: Function with tangent line\n",
        "    ax1.plot(x, y, \"b-\", linewidth=3, label=\"f(x) = x² + 2x + 1\")\n",
        "    ax1.plot(\n",
        "        point_x,\n",
        "        point_y,\n",
        "        \"ro\",\n",
        "        markersize=10,\n",
        "        label=f\"Point ({point_x:.1f}, {point_y:.1f})\",\n",
        "    )\n",
        "    ax1.plot(\n",
        "        tangent_x,\n",
        "        tangent_y,\n",
        "        \"r--\",\n",
        "        linewidth=2,\n",
        "        label=f\"Tangent (slope = {point_grad:.1f})\",\n",
        "    )\n",
        "\n",
        "    # Add arrow showing gradient direction\n",
        "    arrow_scale = 0.3\n",
        "    ax1.arrow(\n",
        "        point_x,\n",
        "        point_y,\n",
        "        arrow_scale,\n",
        "        point_grad * arrow_scale,\n",
        "        head_width=0.1,\n",
        "        head_length=0.1,\n",
        "        fc=\"red\",\n",
        "        ec=\"red\",\n",
        "    )\n",
        "\n",
        "    ax1.set_xlabel(\"x\", fontsize=12)\n",
        "    ax1.set_ylabel(\"f(x)\", fontsize=12)\n",
        "    ax1.set_title(\n",
        "        \"🔍 Gradient as Slope of Tangent Line\", fontsize=14, fontweight=\"bold\"\n",
        "    )\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Gradient function\n",
        "    ax2.plot(x, gradient, \"g-\", linewidth=3, label=\"f'(x) = 2x + 2\")\n",
        "    ax2.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\n",
        "    ax2.axvline(\n",
        "        x=-1, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Zero gradient (x = -1)\"\n",
        "    )\n",
        "    ax2.plot(\n",
        "        point_x, point_grad, \"ro\", markersize=10, label=f\"Gradient at x={point_x:.1f}\"\n",
        "    )\n",
        "\n",
        "    ax2.set_xlabel(\"x\", fontsize=12)\n",
        "    ax2.set_ylabel(\"f'(x)\", fontsize=12)\n",
        "    ax2.set_title(\"📈 Gradient Function\", fontsize=14, fontweight=\"bold\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"🎯 At x = {point_x}:\")\n",
        "    print(f\"   📊 Function value: f({point_x}) = {point_y:.2f}\")\n",
        "    print(f\"   📐 Gradient: f'({point_x}) = {point_grad:.2f}\")\n",
        "    print(\n",
        "        f\"   🧭 Direction: {'Increasing' if point_grad > 0 else 'Decreasing' if point_grad < 0 else 'Flat'}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Visualize 1D gradients\n",
        "visualize_1d_gradient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6064dac",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_2d_gradient():\n",
        "    \"\"\"Visualize gradients in 2D with contour plots and vector fields\"\"\"\n",
        "\n",
        "    # Create a 2D function: f(x,y) = x² + y² (bowl shape)\n",
        "    x = np.linspace(-3, 3, 30)\n",
        "    y = np.linspace(-3, 3, 30)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = X**2 + Y**2\n",
        "\n",
        "    # Compute gradients: ∇f = [2x, 2y]\n",
        "    dZ_dx = 2 * X\n",
        "    dZ_dy = 2 * Y\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Plot 1: 3D surface\n",
        "    ax1 = fig.add_subplot(131, projection=\"3d\")\n",
        "    surf = ax1.plot_surface(X, Y, Z, cmap=\"viridis\", alpha=0.7)\n",
        "    ax1.set_xlabel(\"x\")\n",
        "    ax1.set_ylabel(\"y\")\n",
        "    ax1.set_zlabel(\"f(x,y)\")\n",
        "    ax1.set_title(\"🏔️ 3D Function Surface\\nf(x,y) = x² + y²\", fontweight=\"bold\")\n",
        "\n",
        "    # Plot 2: Contour with gradient vectors\n",
        "    ax2 = fig.add_subplot(132)\n",
        "    contour = ax2.contour(X, Y, Z, levels=15, colors=\"blue\", alpha=0.6)\n",
        "    ax2.clabel(contour, inline=True, fontsize=8)\n",
        "\n",
        "    # Show gradient vectors (downsampled for clarity)\n",
        "    skip = 3\n",
        "    ax2.quiver(\n",
        "        X[::skip, ::skip],\n",
        "        Y[::skip, ::skip],\n",
        "        -dZ_dx[::skip, ::skip],\n",
        "        -dZ_dy[::skip, ::skip],  # Negative for descent direction\n",
        "        color=\"red\",\n",
        "        alpha=0.7,\n",
        "        scale=50,\n",
        "    )\n",
        "\n",
        "    ax2.set_xlabel(\"x\")\n",
        "    ax2.set_ylabel(\"y\")\n",
        "    ax2.set_title(\n",
        "        \"🗺️ Contour + Gradient Vectors\\n(Red arrows = descent direction)\",\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "    ax2.set_aspect(\"equal\")\n",
        "\n",
        "    # Plot 3: Gradient magnitude\n",
        "    ax3 = fig.add_subplot(133)\n",
        "    gradient_magnitude = np.sqrt(dZ_dx**2 + dZ_dy**2)\n",
        "    im = ax3.imshow(\n",
        "        gradient_magnitude, extent=[-3, 3, -3, 3], origin=\"lower\", cmap=\"hot\"\n",
        "    )\n",
        "    ax3.set_xlabel(\"x\")\n",
        "    ax3.set_ylabel(\"y\")\n",
        "    ax3.set_title(\n",
        "        \"🔥 Gradient Magnitude\\n|∇f| = √((∂f/∂x)² + (∂f/∂y)²)\", fontweight=\"bold\"\n",
        "    )\n",
        "    plt.colorbar(im, ax=ax3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"🎯 Key Insights:\")\n",
        "    print(\"   🏔️ Function surface shows the 'landscape' our model needs to navigate\")\n",
        "    print(\"   🧭 Red arrows point in steepest DESCENT direction (where we want to go!)\")\n",
        "    print(\"   🔥 Gradient magnitude shows how 'steep' the slope is at each point\")\n",
        "    print(\"   🎯 At the center (0,0), gradient = [0,0] → we found the minimum!\")\n",
        "\n",
        "\n",
        "# Visualize 2D gradients\n",
        "visualize_2d_gradient()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e101c3f6",
      "metadata": {},
      "source": [
        "## 🔄 Computational Graphs: PyTorch's Secret Weapon\n",
        "\n",
        "PyTorch builds a **computational graph** behind the scenes to track operations and compute gradients automatically!\n",
        "\n",
        "### 🏗️ How It Works:\n",
        "\n",
        "1. **Forward Pass**: Build the graph while computing function values\n",
        "2. **Backward Pass**: Traverse graph in reverse to compute gradients\n",
        "3. **Chain Rule**: Automatically applies chain rule at each node\n",
        "\n",
        "Let's see this in action! 🚀\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0f43c00",
      "metadata": {},
      "source": [
        "## 🚀 PyTorch AutoGrad in Action\n",
        "\n",
        "Now let's see how PyTorch automatically computes gradients for us!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa4409c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_autograd_example():\n",
        "    \"\"\"Demonstrate basic automatic differentiation\"\"\"\n",
        "\n",
        "    print(\"🔧 Simple AutoGrad Example\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Create tensor with gradient tracking\n",
        "    x = torch.tensor(3.0, requires_grad=True)\n",
        "    print(f\"📊 Input: x = {x.item()}\")\n",
        "    print(f\"   requires_grad: {x.requires_grad}\")\n",
        "    print(f\"   grad_fn: {x.grad_fn}\")  # None for leaf nodes\n",
        "\n",
        "    # Define function: f(x) = x² + 2x + 1\n",
        "    y = x**2 + 2 * x + 1\n",
        "    print(f\"\\n🔄 Forward pass:\")\n",
        "    print(f\"   y = x² + 2x + 1 = {y.item():.2f}\")\n",
        "    print(f\"   y.grad_fn: {y.grad_fn}\")\n",
        "    print(f\"   y.requires_grad: {y.requires_grad}\")\n",
        "\n",
        "    # Compute gradient\n",
        "    print(f\"\\n⬅️ Backward pass:\")\n",
        "    y.backward()\n",
        "\n",
        "    print(f\"   dy/dx = {x.grad.item():.2f}\")\n",
        "\n",
        "    # Verify manually: dy/dx = 2x + 2\n",
        "    manual_grad = 2 * x.item() + 2\n",
        "    print(f\"   Manual: dy/dx = 2x + 2 = 2({x.item()}) + 2 = {manual_grad:.2f} ✅\")\n",
        "\n",
        "\n",
        "simple_autograd_example()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa10911e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_variable_autograd():\n",
        "    \"\"\"Demonstrate gradients with multiple variables\"\"\"\n",
        "\n",
        "    print(\"\\n🎲 Multi-Variable AutoGrad\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Create multiple tensors\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "    y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "    print(f\"📊 Inputs:\")\n",
        "    print(f\"   x = {x.item()}, y = {y.item()}\")\n",
        "\n",
        "    # Complex function: f(x,y) = x²y + xy² + 2x + 3y\n",
        "    z = x**2 * y + x * y**2 + 2 * x + 3 * y\n",
        "\n",
        "    print(f\"\\n🔄 Function: f(x,y) = x²y + xy² + 2x + 3y\")\n",
        "    print(f\"   f({x.item()}, {y.item()}) = {z.item():.2f}\")\n",
        "\n",
        "    # Compute gradients\n",
        "    z.backward()\n",
        "\n",
        "    print(f\"\\n⬅️ Gradients:\")\n",
        "    print(f\"   ∂f/∂x = {x.grad.item():.2f}\")\n",
        "    print(f\"   ∂f/∂y = {y.grad.item():.2f}\")\n",
        "\n",
        "    # Manual verification\n",
        "    # ∂f/∂x = 2xy + y² + 2\n",
        "    # ∂f/∂y = x² + 2xy + 3\n",
        "    manual_dx = 2 * x.item() * y.item() + y.item() ** 2 + 2\n",
        "    manual_dy = x.item() ** 2 + 2 * x.item() * y.item() + 3\n",
        "\n",
        "    print(f\"\\n🧮 Manual verification:\")\n",
        "    print(f\"   ∂f/∂x = 2xy + y² + 2 = {manual_dx:.2f} ✅\")\n",
        "    print(f\"   ∂f/∂y = x² + 2xy + 3 = {manual_dy:.2f} ✅\")\n",
        "\n",
        "\n",
        "multi_variable_autograd()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e825021",
      "metadata": {},
      "source": [
        "## 🔗 Chain Rule in Action\n",
        "\n",
        "The real power of automatic differentiation comes from the **chain rule**! Let's see how PyTorch handles complex composite functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24bf77d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def chain_rule_demo():\n",
        "    \"\"\"Demonstrate chain rule with composite functions\"\"\"\n",
        "\n",
        "    print(\"🔗 Chain Rule Demonstration\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Create input\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "    # Build composite function step by step\n",
        "    print(f\"📊 Input: x = {x.item()}\")\n",
        "\n",
        "    # Step 1: u = x² + 1\n",
        "    u = x**2 + 1\n",
        "    print(f\"\\n🔄 Step 1: u = x² + 1 = {u.item():.2f}\")\n",
        "    print(f\"   u.grad_fn: {u.grad_fn}\")\n",
        "\n",
        "    # Step 2: v = sin(u)\n",
        "    v = torch.sin(u)\n",
        "    print(f\"   Step 2: v = sin(u) = sin({u.item():.2f}) = {v.item():.4f}\")\n",
        "    print(f\"   v.grad_fn: {v.grad_fn}\")\n",
        "\n",
        "    # Step 3: y = v²\n",
        "    y = v**2\n",
        "    print(f\"   Step 3: y = v² = {y.item():.6f}\")\n",
        "    print(f\"   y.grad_fn: {y.grad_fn}\")\n",
        "\n",
        "    # Compute gradient using chain rule\n",
        "    print(f\"\\n⬅️ Chain rule: dy/dx = dy/dv × dv/du × du/dx\")\n",
        "    y.backward()\n",
        "\n",
        "    print(f\"   Computed gradient: dy/dx = {x.grad.item():.6f}\")\n",
        "\n",
        "    # Manual chain rule computation\n",
        "    x_val = x.item()\n",
        "    u_val = x_val**2 + 1\n",
        "    v_val = torch.sin(torch.tensor(u_val)).item()\n",
        "\n",
        "    du_dx = 2 * x_val  # d(x²+1)/dx = 2x\n",
        "    dv_du = torch.cos(torch.tensor(u_val)).item()  # d(sin(u))/du = cos(u)\n",
        "    dy_dv = 2 * v_val  # d(v²)/dv = 2v\n",
        "\n",
        "    manual_grad = dy_dv * dv_du * du_dx\n",
        "\n",
        "    print(f\"\\n🧮 Manual chain rule:\")\n",
        "    print(f\"   du/dx = 2x = {du_dx:.2f}\")\n",
        "    print(f\"   dv/du = cos(u) = cos({u_val:.2f}) = {dv_du:.4f}\")\n",
        "    print(f\"   dy/dv = 2v = 2({v_val:.4f}) = {dy_dv:.6f}\")\n",
        "    print(f\"   dy/dx = {dy_dv:.6f} × {dv_du:.4f} × {du_dx:.2f} = {manual_grad:.6f} ✅\")\n",
        "\n",
        "\n",
        "chain_rule_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a123f7f",
      "metadata": {},
      "source": [
        "## ⚠️ Common AutoGrad Gotchas and Best Practices\n",
        "\n",
        "Let's learn about important concepts that every PyTorch user should know!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b8a714",
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference_mode_demo():\n",
        "    \"\"\"Demonstrate torch.inference_mode() context manager\"\"\"\n",
        "\n",
        "    print(\"🚫 torch.inference_mode() Demo\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "    print(f\"📊 Normal operation (gradient tracking ON):\")\n",
        "    y1 = x**2\n",
        "    print(f\"   y = x² = {y1.item():.2f}\")\n",
        "    print(f\"   y.requires_grad: {y1.requires_grad}\")\n",
        "    print(f\"   y.grad_fn: {y1.grad_fn}\")\n",
        "\n",
        "    print(f\"\\n🚫 With torch.inference_mode() (gradient tracking OFF):\")\n",
        "    with torch.inference_mode():\n",
        "        y2 = x**2\n",
        "        print(f\"   y = x² = {y2.item():.2f}\")\n",
        "        print(f\"   y.requires_grad: {y2.requires_grad}\")\n",
        "        print(f\"   y.grad_fn: {y2.grad_fn}\")\n",
        "\n",
        "    print(f\"\\n💡 Use cases for inference_mode():\")\n",
        "    print(f\"   🔍 Model inference (evaluation)\")\n",
        "    print(f\"   📊 Computing metrics\")\n",
        "    print(f\"   💾 Saving memory during validation\")\n",
        "\n",
        "\n",
        "inference_mode_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6c1da3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def detach_demo():\n",
        "    \"\"\"Demonstrate tensor.detach() method\"\"\"\n",
        "\n",
        "    print(\"\\n🔗 tensor.detach() Demo\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    x = torch.tensor(3.0, requires_grad=True)\n",
        "    y = x**2 + 2 * x\n",
        "\n",
        "    print(f\"📊 Original tensor:\")\n",
        "    print(f\"   y = {y.item():.2f}\")\n",
        "    print(f\"   y.requires_grad: {y.requires_grad}\")\n",
        "    print(f\"   y.grad_fn: {y.grad_fn}\")\n",
        "\n",
        "    # Detach from computational graph\n",
        "    y_detached = y.detach()\n",
        "\n",
        "    print(f\"\\n🔗 Detached tensor:\")\n",
        "    print(f\"   y_detached = {y_detached.item():.2f}\")\n",
        "    print(f\"   y_detached.requires_grad: {y_detached.requires_grad}\")\n",
        "    print(f\"   y_detached.grad_fn: {y_detached.grad_fn}\")\n",
        "\n",
        "    print(f\"\\n💡 Use cases for detach():\")\n",
        "    print(f\"   🛑 Stop gradients at specific points\")\n",
        "    print(f\"   🔄 Implement custom backward behavior\")\n",
        "    print(f\"   📊 Extract values for logging/plotting\")\n",
        "\n",
        "\n",
        "detach_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9d7d682",
      "metadata": {},
      "source": [
        "## 🧠 Neural Network Gradients in Practice\n",
        "\n",
        "Let's see how gradients work in a real neural network!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38584d65",
      "metadata": {},
      "outputs": [],
      "source": [
        "def neural_network_gradients():\n",
        "    \"\"\"Demonstrate gradients in a simple neural network\"\"\"\n",
        "\n",
        "    print(\"🧠 Neural Network Gradient Demo\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Create a simple 2-layer neural network\n",
        "    class SimpleNN(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super(SimpleNN, self).__init__()\n",
        "            self.layer1 = torch.nn.Linear(2, 3)\n",
        "            self.layer2 = torch.nn.Linear(3, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.layer1(x))\n",
        "            x = self.layer2(x)\n",
        "            return x\n",
        "\n",
        "    # Initialize network and data\n",
        "    torch.manual_seed(42)\n",
        "    model = SimpleNN()\n",
        "    x = torch.tensor([[1.0, 2.0]], requires_grad=False)  # Input doesn't need gradients\n",
        "    target = torch.tensor([[3.0]])\n",
        "\n",
        "    print(f\"📊 Network architecture:\")\n",
        "    print(\n",
        "        f\"   Input: {x.shape} → Layer1: {list(model.layer1.parameters())[0].shape} → ReLU → Layer2: {list(model.layer2.parameters())[0].shape} → Output\"\n",
        "    )\n",
        "\n",
        "    # Forward pass\n",
        "    prediction = model(x)\n",
        "    loss = torch.nn.functional.mse_loss(prediction, target)\n",
        "\n",
        "    print(f\"\\n🔄 Forward pass:\")\n",
        "    print(f\"   Input: {x.squeeze().tolist()}\")\n",
        "    print(f\"   Prediction: {prediction.item():.4f}\")\n",
        "    print(f\"   Target: {target.item():.1f}\")\n",
        "    print(f\"   Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Check gradients before backward\n",
        "    print(f\"\\n📊 Gradients before backward pass:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"   {name}: {param.grad}\")\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    print(f\"\\n⬅️ Gradients after backward pass:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            print(\n",
        "                f\"   {name}: shape={param.grad.shape}, mean={param.grad.mean().item():.6f}\"\n",
        "            )\n",
        "\n",
        "    print(\n",
        "        f\"\\n🎯 These gradients tell us how to update each parameter to reduce the loss!\"\n",
        "    )\n",
        "\n",
        "\n",
        "neural_network_gradients()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97a6cdc",
      "metadata": {},
      "source": [
        "## 🎮 Interactive Gradient Visualization\n",
        "\n",
        "Let's create an interactive demonstration of how gradients change during optimization!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b407739a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_best_practices():\n",
        "    \"\"\"Summary of gradient best practices and key concepts\"\"\"\n",
        "\n",
        "    print(\"🎓 Gradient & AutoGrad Best Practices\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    print(\"✅ DO:\")\n",
        "    print(\"   🎯 Set requires_grad=True for parameters you want to optimize\")\n",
        "    print(\"   🧹 Use optimizer.zero_grad() to clear gradients before each step\")\n",
        "    print(\"   🚫 Use torch.inference_mode() during inference to save memory\")\n",
        "    print(\"   🔍 Check gradient values for debugging (NaN, explosion, vanishing)\")\n",
        "    print(\"   📏 Use gradient clipping for unstable training\")\n",
        "    print(\"   🎛️ Detach tensors when you want to stop gradient flow\")\n",
        "\n",
        "    print(\"\\n❌ DON'T:\")\n",
        "    print(\"   ⚠️ Don't use in-place operations on tensors with gradients\")\n",
        "    print(\"   🔄 Don't forget to call loss.backward() to compute gradients\")\n",
        "    print(\"   🧠 Don't accumulate gradients across batches unless intended\")\n",
        "    print(\"   💾 Don't keep computation graphs longer than necessary\")\n",
        "    print(\"   🔁 Don't call backward() multiple times without retain_graph=True\")\n",
        "\n",
        "    print(\"\\n🎯 Key Concepts Recap:\")\n",
        "\n",
        "    concepts = {\n",
        "        \"🔄 Computational Graph\": \"Dynamic graph built during forward pass\",\n",
        "        \"⬅️ Backward Pass\": \"Traverses graph in reverse, applying chain rule\",\n",
        "        \"📐 Gradients\": \"Partial derivatives showing direction of steepest ascent\",\n",
        "        \"🧮 Chain Rule\": \"Automatically applied to compute composite derivatives\",\n",
        "        \"🎛️ requires_grad\": \"Flag to enable gradient computation\",\n",
        "        \"🚫 inference_mode()\": \"Context manager to disable gradient computation\",\n",
        "        \"🔗 detach()\": \"Creates new tensor disconnected from graph\",\n",
        "        \"📈 Accumulation\": \"Gradients add up with multiple backward() calls\",\n",
        "    }\n",
        "\n",
        "    for concept, description in concepts.items():\n",
        "        print(f\"   {concept}: {description}\")\n",
        "\n",
        "    print(\"\\n🚀 Next Steps:\")\n",
        "    print(\"   📚 Study optimization algorithms (SGD, Adam, etc.)\")\n",
        "    print(\"   🧠 Learn about different loss functions\")\n",
        "    print(\"   🎯 Practice with real neural network training\")\n",
        "    print(\n",
        "        \"   🔬 Explore advanced techniques (gradient clipping, learning rate scheduling)\"\n",
        "    )\n",
        "    print(\"   🏗️ Build custom models and understand their gradient flow\")\n",
        "\n",
        "\n",
        "gradient_best_practices()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a477bc0d",
      "metadata": {},
      "source": [
        "## 🎮 Practice Exercises\n",
        "\n",
        "Try these exercises to test your understanding of gradients and automatic differentiation!\n",
        "\n",
        "### Exercise 1: Manual vs AutoGrad\n",
        "\n",
        "Create a function `f(x, y) = x³ + 2xy + y²` and:\n",
        "\n",
        "1. Compute gradients manually\n",
        "2. Verify with PyTorch autograd\n",
        "3. Plot the function and gradient vectors\n",
        "\n",
        "### Exercise 2: Neural Network Gradients\n",
        "\n",
        "Build a 3-layer neural network and:\n",
        "\n",
        "1. Analyze gradient magnitudes in each layer\n",
        "2. Implement gradient clipping\n",
        "3. Visualize how gradients change during training\n",
        "\n",
        "### Exercise 3: Custom Activation Function\n",
        "\n",
        "Implement a custom activation function with:\n",
        "\n",
        "1. Custom forward pass\n",
        "2. Custom backward pass\n",
        "3. Compare with built-in functions\n",
        "\n",
        "### Exercise 4: Gradient Flow Analysis\n",
        "\n",
        "Create a deep network and:\n",
        "\n",
        "1. Track gradient magnitudes across layers\n",
        "2. Identify vanishing/exploding gradients\n",
        "3. Implement solutions (residual connections, normalization)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181dafd6",
      "metadata": {},
      "source": [
        "## 🏆 Congratulations!\n",
        "\n",
        "You've mastered the fundamentals of **gradients and automatic differentiation** in PyTorch! 🎉\n",
        "\n",
        "### What You've Learned:\n",
        "\n",
        "🎯 **Core Concepts:**\n",
        "\n",
        "- What gradients represent and why they're crucial for ML\n",
        "- How computational graphs enable automatic differentiation\n",
        "- The chain rule and its automatic application\n",
        "\n",
        "🛠️ **Practical Skills:**\n",
        "\n",
        "- Using `requires_grad`, `backward()`, and gradient management\n",
        "- Debugging gradient-related issues\n",
        "- Implementing custom autograd functions\n",
        "- Best practices for gradient computation\n",
        "\n",
        "🧠 **Advanced Techniques:**\n",
        "\n",
        "- Higher-order gradients for optimization algorithms\n",
        "- Gradient visualization and analysis\n",
        "- Memory-efficient gradient computation\n",
        "\n",
        "### Ready for the Next Challenge?\n",
        "\n",
        "With this solid foundation in gradients, you're prepared to:\n",
        "\n",
        "- 🏗️ Build and train complex neural networks\n",
        "- 🎯 Understand optimization algorithms deeply\n",
        "- 🔧 Debug training issues effectively\n",
        "- 🚀 Implement cutting-edge ML techniques\n",
        "\n",
        "**Next up**: Custom Datasets and Data Loading - where you'll learn to work with real-world data! 📊\n",
        "\n",
        "Keep practicing and experimenting - gradients are the foundation of all modern ML! 💪\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
