{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae3d170",
   "metadata": {},
   "source": [
    "# 03. Neural Networks - Theory and Mathematics\n",
    "\n",
    "Welcome to the fascinating world of **neural networks**! üß†\n",
    "\n",
    "Neural networks are the foundation of modern AI and deep learning. They're inspired by how our brains work, with interconnected neurons processing information. In this notebook, we'll explore:\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "\n",
    "- What neural networks are and how they work\n",
    "- The mathematical foundations behind neural networks\n",
    "- Different types of activation functions and their purposes\n",
    "- How neural networks learn through backpropagation\n",
    "- When to use neural networks vs. traditional machine learning\n",
    "\n",
    "## üß© What You'll Build\n",
    "\n",
    "- Interactive visualizations of neural network architecture\n",
    "- Mathematical demonstrations of forward propagation\n",
    "- Activation function comparisons with visual examples\n",
    "- A simple neural network from scratch (conceptually)\n",
    "\n",
    "Let's start with a real-world example to make this concrete! üöÄ\n",
    "\n",
    "## üéì Real-World Example: Exam Prediction\n",
    "\n",
    "Imagine you want to predict if a student will pass an exam. You have the following data:\n",
    "\n",
    "- **Hours studied** (x‚ÇÅ) - Numerical: 0-40 hours\n",
    "- **Previous knowledge** (x‚ÇÇ) - Scale: 1-10\n",
    "- **Sleep hours** (x‚ÇÉ) - Numerical: 4-12 hours\n",
    "- **Stress level** (x‚ÇÑ) - Scale: 1-10\n",
    "\n",
    "Our neural network will take these four inputs and predict: **Pass (1)** or **Fail (0)**\n",
    "\n",
    "Unlike traditional algorithms that use simple rules, neural networks can learn complex patterns and relationships between these features automatically!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008e826",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Neural Network Architecture\n",
    "\n",
    "A neural network is like a **digital brain** made up of layers of artificial neurons. Each neuron:\n",
    "\n",
    "1. Takes multiple inputs\n",
    "2. Applies mathematical transformations\n",
    "3. Produces an output\n",
    "4. Passes that output to the next layer\n",
    "\n",
    "Think of it like an assembly line where each worker (neuron) does a specific job and passes the result to the next worker!\n",
    "\n",
    "<img src=\"../09_images/03_neural_network_architecure.png\" alt=\"Neural Network Architecture\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2951f",
   "metadata": {},
   "source": [
    "## üéØ How Neural Networks Learn\n",
    "\n",
    "The magic of neural networks lies in their ability to **learn from data**. But how exactly do they learn? Let's break it down!\n",
    "\n",
    "### Step 1: Weight Initialization\n",
    "\n",
    "Each connection between neurons has a weight, which determines how much influence one neuron has on another. Initially, these weights are set randomly.\n",
    "\n",
    "<img src=\"../09_images/03_step_1_initialize_weights.png\" alt=\"Weight Initialization\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bc011",
   "metadata": {},
   "source": [
    "### Step 2: Bias Initialization\n",
    "\n",
    "Each neuron also has a bias, which allows it to shift the activation function. Biases are also initialized randomly.\n",
    "\n",
    "<img src=\"../09_images/03_step_2_initialize_bias.png\" alt=\"Bias Initialization\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f7b02f",
   "metadata": {},
   "source": [
    "### Step 3: Forward Propagation/Pass\n",
    "\n",
    "In forward propagation, the input data is passed through the network layer by layer. Each neuron applies its weights and bias to the inputs, then passes the result through an activation function to produce an output.\n",
    "\n",
    "<img src=\"../09_images/03_step_3_forward_pass.png\" alt=\"Forward Propagation\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ac825",
   "metadata": {},
   "source": [
    "### Step 4: Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. Common activation functions include:\n",
    "\n",
    "- Sigmoid\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Tanh (Hyperbolic Tangent)\n",
    "\n",
    "<img src=\"../09_images/03_step_4_activation_function.png\" alt=\"Activation Functions\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85baba0",
   "metadata": {},
   "source": [
    "### Why Activation Functions Matter\n",
    "\n",
    "Activation functions are crucial because they allow the network to learn non-linear relationships. Without them, the network would behave like a linear model, limiting its ability to capture complex patterns in the data.\n",
    "\n",
    "<img src=\"../09_images/03_why_activation_functions.png\" alt=\"Why Activation Functions Matter\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3e1de",
   "metadata": {},
   "source": [
    "### Step 5: Loss Calculation\n",
    "\n",
    "After forward propagation, the network's output is compared to the actual target (e.g., whether the student passed or failed). This comparison produces a **loss** value, which indicates how far off the prediction was.\n",
    "\n",
    "The loss is calculated using a **loss function**, which quantifies the difference between the predicted output and the actual target. Common loss functions include:\n",
    "\n",
    "- Mean Squared Error (MSE) for regression tasks\n",
    "- Cross-Entropy Loss for classification tasks\n",
    "\n",
    "<img src=\"../09_images/03_step_5_calculate_loss.png\" alt=\"Loss Calculation\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b500a6",
   "metadata": {},
   "source": [
    "### Step 6: Backpropagation\n",
    "\n",
    "In backpropagation, the loss is propagated back through the network to update the weights and biases. This is done using the chain rule of calculus, allowing the model to learn from its mistakes.\n",
    "\n",
    "<img src=\"../09_images/03_step_6_backpropagation.png\" alt=\"Backpropagation\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118b607",
   "metadata": {},
   "source": [
    "### Summary of Steps\n",
    "\n",
    "1. **Data Preparation**: Collect and preprocess the data.\n",
    "2. **Model Initialization**: Define the neural network architecture.\n",
    "3. **Forward Propagation**: Pass the input data through the network to obtain predictions.\n",
    "4. **Activation Functions**: Apply activation functions to introduce non-linearity.\n",
    "5. **Loss Calculation**: Compute the loss by comparing predictions with actual targets.\n",
    "6. **Backpropagation**: Update the model's weights and biases based on the loss.\n",
    "\n",
    "<img src=\"../09_images/03_neural_network_learning.png\" alt=\"Summary of Steps\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cedfa4e",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Types of Neural Networks\n",
    "\n",
    "Not all neural networks are the same! Different problems need different architectures. Let's explore the main types:\n",
    "\n",
    "## üîÑ Feedforward Neural Network (FNN)\n",
    "\n",
    "**Description:**  \n",
    "A basic type of neural network where data flows in one direction, from input to output.\n",
    "\n",
    "| Feature     | Detail                                                             |\n",
    "| ----------- | ------------------------------------------------------------------ |\n",
    "| ‚úÖ Best For | Structured/tabular data                                            |\n",
    "| üõ†Ô∏è Examples | Exam grade prediction, Housing price prediction, Medical diagnosis |\n",
    "\n",
    "- **Structure:** Input layer ‚Üí Hidden layers ‚Üí Output layer\n",
    "- **Flow:** One-way (no loops or cycles)\n",
    "\n",
    "## üñºÔ∏è Convolutional Neural Network (CNN)\n",
    "\n",
    "**Description:**  \n",
    "Designed to process grid-like data such as images.\n",
    "\n",
    "| Feature     | Detail                                                         |\n",
    "| ----------- | -------------------------------------------------------------- |\n",
    "| ‚úÖ Best For | Images, spatial data                                           |\n",
    "| üõ†Ô∏è Examples | Photo classification, Face recognition, Medical image analysis |\n",
    "\n",
    "- **Structure:** Convolution layers ‚Üí Pooling layers ‚Üí Fully connected layers ‚Üí Output\n",
    "- **Special Ability:** Captures local spatial hierarchies in images.\n",
    "\n",
    "## üîÅ Recurrent Neural Network (RNN)\n",
    "\n",
    "**Description:**  \n",
    "Designed for sequential or time-dependent data.\n",
    "\n",
    "| Feature     | Detail                                                           |\n",
    "| ----------- | ---------------------------------------------------------------- |\n",
    "| ‚úÖ Best For | Sequential data                                                  |\n",
    "| üõ†Ô∏è Examples | Language translation, Stock price prediction, Speech recognition |\n",
    "\n",
    "- **Structure:** Loops allowing hidden states to persist across time steps.\n",
    "- **Limitation:** Can struggle with long sequences (solved by LSTM/GRU).\n",
    "\n",
    "## üß† Long Short-Term Memory (LSTM)\n",
    "\n",
    "**Description:**  \n",
    "An advanced form of RNN designed to better capture long-term dependencies.\n",
    "\n",
    "| Feature     | Detail                                                      |\n",
    "| ----------- | ----------------------------------------------------------- |\n",
    "| ‚úÖ Best For | Long-sequence data, time series                             |\n",
    "| üõ†Ô∏è Examples | Long text generation, Weather forecasting, Music generation |\n",
    "\n",
    "- **Structure:** Memory cells, gates (input, forget, output).\n",
    "- **Benefit:** Avoids vanishing gradient problem typical in RNNs.\n",
    "\n",
    "## ‚öôÔ∏è Gated Recurrent Unit (GRU)\n",
    "\n",
    "**Description:**  \n",
    "A simpler alternative to LSTM with similar benefits.\n",
    "\n",
    "| Feature     | Detail                           |\n",
    "| ----------- | -------------------------------- |\n",
    "| ‚úÖ Best For | Sequential data, faster training |\n",
    "| üõ†Ô∏è Examples | Chatbots, Time-series prediction |\n",
    "\n",
    "- **Structure:** Combines hidden state and memory cell in a more efficient way.\n",
    "- **Benefit:** Fewer parameters than LSTM, faster to train.\n",
    "\n",
    "## üöÄ Transformer\n",
    "\n",
    "**Description:**  \n",
    "State-of-the-art architecture for processing complex sequences.\n",
    "\n",
    "| Feature     | Detail                                           |\n",
    "| ----------- | ------------------------------------------------ |\n",
    "| ‚úÖ Best For | Complex sequences, language tasks                |\n",
    "| üõ†Ô∏è Examples | ChatGPT, Document summarization, Code generation |\n",
    "\n",
    "- **Structure:** Attention mechanism, Encoder-Decoder blocks.\n",
    "- **Benefit:** Parallel processing, handles long sequences better than RNN.\n",
    "\n",
    "## ü™Ñ Vision Transformer (ViT)\n",
    "\n",
    "**Description:**  \n",
    "Applies Transformer architecture to image data.\n",
    "\n",
    "| Feature     | Detail                                           |\n",
    "| ----------- | ------------------------------------------------ |\n",
    "| ‚úÖ Best For | Image classification (alternative to CNN)        |\n",
    "| üõ†Ô∏è Examples | Object detection, Medical imaging, Face analysis |\n",
    "\n",
    "- **Structure:** Splits image into patches, processes as a sequence using attention.\n",
    "- **Trend:** Gaining popularity for large-scale image tasks.\n",
    "\n",
    "## üîÑ Autoencoder\n",
    "\n",
    "**Description:**  \n",
    "Learns to compress and reconstruct data.\n",
    "\n",
    "| Feature     | Detail                                            |\n",
    "| ----------- | ------------------------------------------------- |\n",
    "| ‚úÖ Best For | Data compression, anomaly detection               |\n",
    "| üõ†Ô∏è Examples | Noise reduction, Fraud detection, Image denoising |\n",
    "\n",
    "- **Structure:** Encoder ‚Üí Bottleneck ‚Üí Decoder.\n",
    "- **Benefit:** Useful for unsupervised learning tasks.\n",
    "\n",
    "## üìä Comparison Table\n",
    "\n",
    "| Network Type       | Best For                 | Example Use Cases                                |\n",
    "| ------------------ | ------------------------ | ------------------------------------------------ |\n",
    "| Feedforward        | Structured data          | Exam grade prediction, Housing price prediction  |\n",
    "| CNN                | Images & spatial data    | Photo classification, Face recognition           |\n",
    "| RNN                | Sequential data          | Language translation, Stock prediction           |\n",
    "| LSTM               | Long-sequence data       | Weather forecasting, Long text generation        |\n",
    "| GRU                | Sequential data (faster) | Chatbots, Time-series prediction                 |\n",
    "| Transformer        | Complex sequences        | ChatGPT, Document summarization, Code generation |\n",
    "| Vision Transformer | Image classification     | Object detection, Face analysis                  |\n",
    "| Autoencoder        | Data compression         | Anomaly detection, Image denoising               |\n",
    "\n",
    "## ‚úÖ Choosing the Right Neural Network\n",
    "\n",
    "- **Feedforward (FNN):** Start here for most simple tabular data problems.\n",
    "- **CNN:** Essential for any image-related task.\n",
    "- **RNN / LSTM / GRU:** When order or sequence matters in your data.\n",
    "- **Transformer:** Best choice for complex sequences and modern NLP tasks.\n",
    "- **Vision Transformer:** Emerging standard for image-related tasks at scale.\n",
    "- **Autoencoder:** Useful for unsupervised tasks like anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afee717",
   "metadata": {},
   "source": [
    "## üöÄ What's Next?\n",
    "\n",
    "Now that you understand the theory behind neural networks, you're ready to build them!\n",
    "\n",
    "### üìö In the Next Notebook: [04. Neural Networks in PyTorch](04_neural_networks_in_pytorch.ipynb)\n",
    "\n",
    "You'll learn to:\n",
    "\n",
    "- **Build** your first neural network with PyTorch\n",
    "- **Train** it on real data\n",
    "- **Evaluate** its performance\n",
    "- **Visualize** the learning process\n",
    "- **Improve** the model step by step\n",
    "\n",
    "### üéØ Key Takeaways from This Notebook\n",
    "\n",
    "‚úÖ **Neural networks are universal function approximators** - they can learn almost any pattern!\n",
    "\n",
    "‚úÖ **Layers work together**: Input ‚Üí Hidden (processing) ‚Üí Output (prediction)\n",
    "\n",
    "‚úÖ **Neurons perform math**: Weighted sum + bias + activation function\n",
    "\n",
    "‚úÖ **Learning happens through backpropagation**: Adjust weights based on errors\n",
    "\n",
    "‚úÖ **Different architectures solve different problems**: Feedforward, CNN, RNN, Transformers\n",
    "\n",
    "‚úÖ **Real-world impact is massive**: From your smartphone to self-driving cars!\n",
    "\n",
    "### ü§î Test Your Understanding\n",
    "\n",
    "Before moving on, can you answer these questions?\n",
    "\n",
    "1. What are the three main components of a neuron's computation?\n",
    "2. Why do we need activation functions? What would happen without them?\n",
    "3. Which type of neural network would you use for analyzing customer reviews?\n",
    "4. How does a neural network \"learn\" from its mistakes?\n",
    "\n",
    "Ready to start coding? Let's build some neural networks! üî•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d085f",
   "metadata": {},
   "source": [
    "## üìñ Additional Resources\n",
    "\n",
    "### üé• Recommended Videos\n",
    "\n",
    "- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) - Beautiful visual explanations\n",
    "- [Andrew Ng's Deep Learning Course](https://www.coursera.org/specializations/deep-learning) - Comprehensive and rigorous\n",
    "\n",
    "### üìö Further Reading\n",
    "\n",
    "- [Deep Learning Book](https://www.deeplearningbook.org/) by Ian Goodfellow (Free online)\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen\n",
    "\n",
    "### üõ†Ô∏è Interactive Tools\n",
    "\n",
    "- [TensorFlow Playground](https://playground.tensorflow.org/) - Experiment with neural networks in your browser\n",
    "- [Distill.pub](https://distill.pub/) - Interactive machine learning explanations\n",
    "\n",
    "### üß† Advanced Topics (For Later)\n",
    "\n",
    "- Batch Normalization\n",
    "- Dropout and Regularization\n",
    "- Advanced Optimizers (Adam, RMSprop)\n",
    "- Transfer Learning\n",
    "- Generative Adversarial Networks (GANs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d8493",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
